\chapter{Implementation}
\label{chap:implementation}
In this chapter, the implementation of the network stack using the
pipelined design from \autoref{chap:design} is outlined and described,
the application of SME detailed and evaluated, and lastly, the viability
of the system on an FPGA is discussed.\\

The network stack is implemented in C\# using the C\# version of SME, which is,
at the moment of writing, more mature and feature-rich. The current version of
the implementation supports most of the absolutely vital parts of the IPv4
protocol, as well as the UDP protocol, as specified by RFC 1122\cite{RFC1122}.
Although work has been carried out in order to ensure that additional protocols
can be implemented without obstructions, no additional protocols are supported
at the moment.\\
The solution is fairly well-divided into 3 different types of components,
relating closely to those of SME: processes, buffers, and busses. The most
interesting parts of these components will be described in further detail in the
following sections.


\section{Processes}
The processes are arguably the most vital part of the system, as they provide
the computation and "processing" on the in- and out-going packets.
It is important to note that although there are many other types of "processes",
in the network, such as the buffers, we will mainly refer to the modules doing
actual business-logic as "processes" \notelookup{Did we?} \footnote{These processes are not to be
confused with SME processes, which are used for the implementation of both the
buffers and processes.}.

The essential processes in the network are represented as light-grey boxes in
the \autoref{fig:final_design}. These processes are \texttt{Internet\_In},
\texttt{Internet\_Out}, and \texttt{Transport}.


\subsection{State-machines}
Network communication consists of countless different packets, formats,
protocols, combinations of flags and settings, and even errors and corrupted
bits. The processes in the network have to take on a manifold of jobs in order
to handle all these scenarios, which sadly cannot be handled with a simple
combinational logic circuit. To operate under these various conditions, these
processes are modelled as finite state machines, maintaining a single state at
all times.\\
The processes have a lot of similar states, such as \texttt{Idle}, \texttt{Receive},
\texttt{Pass}, or \texttt{Send}, but these can work very differently, as shown
in the following sections. Before moving on to describing the state-machines of
the 3 processes, it is crucial to understand how these can be modelled in SME.

\subsubsection{SME process execution flow}
To implement a process in SME, the C\# class has to inherit from
either the \texttt{StateProcess} abstract class, or the more simple
\texttt{SimpleProcess} class.
The latter class is, as its name states, a simpler version of the former. This
class implements an \texttt{OnTick()} method, which is invoked once for every
clock-cycle.\\
The more advanced, but also more capable \texttt{StateProcess}
class provides an abstract method \texttt{Run} which is to be overriden and filled
with the code desired to be run in the process. The interesting feature
about this method is that it is asynchronous, meaning that the code can
execute other tasks while waiting for resources, such as functions,
to return. In this case, this asynchronous feature is used to give
the programmer ability to split the function into multiple segments,
separated by the clock signal.\\
\autoref{fig:example_fsm} compares these two approaches for the same
finite state-machine with 3 consequtive states.

The "synchronous" approach using a \texttt{SimpleProcess} in subfigure
\ref{fig:sme_example_process_sync_code}
\notemark{Fix autoref}
has to implement a
variable tracking the current state of the process. On each new clock, this
state has to be analysed and the inteded function to be called based on the
value. This approach requires a lot of approach and boilerplate code, especialy
if there are several states.\\
The asynchronous approach on subfigure
\ref{fig:sme_example_process_async_code}
\notemark{Fix autoref}
on the other hand can do with only
single \texttt{Run()} method split into three parts -- A, B, and C.
After each code-segment, the process waits for the clock signal, and
continues with the execution of the next segment.  This functionality
gives the programmer a very granular control of the way a process
works, how it is split into multiple steps on the hardware, while
maintaining simplicity, as seen on the statemachine diagram on subfigure
\ref{fig:sme_example_process_fsm}.
\notemark{Fix autoref}

\begin{figure*}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
%\begin{lstlisting}[language={[Sharp]C}]
\begin{mintedcsharp}
public class SomeProcess : StateProcess
{
  private override async Task OnTickAsync()
  {
    a();
    await ClockAsync();
    b();
    await ClockAsync();
    c();
    await ClockAsync();
  }
}
\end{mintedcsharp}
%\end{lstlisting}
        \caption{Example using inheriting from \texttt{Process}, using the \texttt{Run()} asynchronous method.}
	\label{fig:sme_example_process_async_code}
    \end{subfigure}
\hfill
    \begin{subfigure}[b]{0.3\textwidth}
%\begin{lstlisting}[language={[Sharp]C}]
\begin{mintedcsharp}
public class SomeProcess : SimpleProcess
{
// Initial state
state = A;

protected override void OnTick()
{
  switch(state) {
    case A:
      a();
      state = B;
    case B:
      b();
      state = C;
    case C:
      c();
      state = A;
  }
}
\end{mintedcsharp}
%\end{lstlisting}
	\caption{Example pseudocode of using a "synchronous" \texttt{SimpleProcess}.}
	\label{fig:sme_example_process_sync_code}
    \end{subfigure}
\hfill
 \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[scale=0.45]{implementation/empty_process_fsm.eps}
        \caption{The statemachine resulting from both code-examples. \\}
 	\label{fig:sme_example_process_fsm}
\end{subfigure}
    \caption{A simple state-machine implemented in the asynchronous (left) and
synchronous (right) approach in SME using C\#}
    \label{fig:example_fsm}
\end{figure*}


\subsubsection{\texttt{Internet Out} state machine}
This way of modelling a process in SME first the \texttt{Internet Out} process
very well, as it only has one responsibility, which is reading outgoing segments
and wrapping them in an Internet header. The \autoref{fig:internet_out_implementation} shows the pseudo-code and state-machine for the \texttt{Internet Out}
process. This process was easy to model and implement, because it only has one
input and one output, and the state-changes are simple and intuitive.

\begin{figure*}[htpb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
%\begin{lstlisting}[language={[Sharp]C}]
\begin{mintedcsharp}
public partial class InternetOut: StateProcess
{
public override async Task OnTickAsync()
{
  while segment_available() {
    pass_segment();
    await ClockAsync();
  }

  while header_available() {
    send_header();
    await ClockAsync();
  }
}
}
\end{mintedcsharp}
%\end{lstlisting}
        \caption{Pseudo-code for the \texttt{InternetOut} process.\\}
	\label{fig:internet_out_pseudocode}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=0.45]{implementation/internet_out_fsm.eps}
        \caption{The statemachine for the \texttt{InternetOut} process}
 	\label{fig:internet_out_fsm}
    \end{subfigure}%
    \caption{The implementation of the \texttt{InternetOut} process.}
    \label{fig:internet_out_implementation}
\end{figure*}


\subsubsection{\texttt{Internet In} and \texttt{Transport} state machines}
Unfortunately\notelookup{Hvorfor "Unfortunately"},
The state-machine of \texttt{Internet In} is probably the most simple of all the
state-machines, as it can effectively only read new packets from the Link-layer,
and pass it along the pipeline. Although it might be desireable for the Internet
layer to send control packets out to the network, this is not supported in the
current build.\noteinfo{Mere angående transport state machine?}



\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{implementation/internet_in_fsm.eps}
        \caption{The \texttt{Internet In} state machine}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.45]{implementation/transport_fsm.eps}
        \caption{The \texttt{Transport} state machine}
    \end{subfigure}%

    \caption{Statemachines for \texttt{Internet In} and \texttt{Transport}}
    \label{fig:statemachines_internetin_transport}
\end{figure*}


\section{Buffers}
Problems such as packet fragmentation and out of order insertion are solved in
the memory buffers.\\
All of the buffers needs to handle input and output of
memory at the same time. If not, the system would slow down by a factor of two,
since the input would need to wait for the output to finish submitting, and
vice versa. This requires the underlying memory to handle read and write in
parallel. \\
Each of the memory buffers have slightly different variations because
there are different problems to solve in all of them. In general, the problems
are the following: \notemark{Mention latency stuff}

\begin{itemize}
    \item \textbf{Fragmentation/Segmentation}\\
    In \texttt{Segment In} and \texttt{Data In} there are segmentation. In
    \texttt{Segment In} IP fragments may arrive out of order, and is therefore only
    sent through the system when all segments are received. \texttt{Data In} is
    essentially the same, just with other protocols such as TCP.\\
    In both cases we want the order to be concurrent. Packets that are not
    segmented is in FIFO order. Segmented packets in \texttt{Segment In} are
    held back, until all segments are received. Then all data segments are sent
    as a single block to \texttt{Transport}. In \texttt{Data In}, the segments are
    handled as a stream, where the last valid segment ID is known. The buffer
    only sends data when the last valid segment is updated. This way, the buffer
    can contain segments with an higher ID than the valid, and only send the ones
    that are smaller than the currently valid ID.


    \item \textbf{Unknown size}\\
    Some of the buffers do not get information about how much data has to be
    allocated. This happens in \texttt{Segment Out} and \texttt{Data Out}.
    In \texttt{Data Out} we do not know how much data the user sends into
    the system. (see: \autoref{subsubsec:interface_control}). In
    \texttt{Segment Out} we do not know some of the packet sizes, since
    \texttt{Transport} may not know how big the packet is going to be. For
    example when constructing a packet that depends on information that is
    unknown at the beginning of the passing state, but known later.
    \notemark{Ask Jan}


    \item \textbf{Out-of-order submission}\\
    In Some cases the header of an packet can only be created after the data
    has been received. An example of this is the calculation of the checksum.\\
    This feature is required on \texttt{Frame In} and \texttt{Frame Out}.
    This only applies to buffers where data is sent out of the network stack.

    \item \textbf{Data ready}\\
    When the buffer indicates that data is ready to be read, the next clock
    should also be ready and contain data. If not, the consumer would request
    data, wait at least 2 clocks for the data, and then request new data,
    slowing down the data transfer process significantly.

\end{itemize}
An oversight of this can be seen in \autoref{tab:buffer_requirements}.\\
\begin{table}[htpb]
  \begin{center}
      \begin{tabular}{l|c|c|c|c|}
          & \tablerot{Fragmentation}
          & \tablerot{\makecell{Unknown size}}
          & \tablerot{\makecell{Out-of-order \\ submission}}
          & \tablerot{\makecell{Data ready}} \\\hline
          \texttt{Frame Out}   &            &             & \checkmark & \checkmark \\ \hline
          \texttt{Segment In}  & \checkmark &             &            & \checkmark \\ \hline
          \texttt{Segment Out} &            & \checkmark  & \checkmark & \checkmark \\ \hline
          \texttt{Data In}     & \checkmark &             &            & \checkmark \\ \hline
          \texttt{Data Out}    &            & \checkmark  &            & \checkmark \\ \hline
      \end{tabular}
  \end{center}
  \caption{The requirements for the buffers} \label{tab:buffer_requirements}
\end{table}
\subsection{Components}
This section describes the components briefly to give a better overview of the
general structures of the buffers. The specific components are described in
detail in the following chapters.\\
To solve fragmentation the system uses "segments" in the memory.
A segment is an abstract structure consisting of metadata and two memory
addresses pointing to the start and end of the actual data.\\
To handle these segments, two interfaces are created, one to handle fixed size
allocations, and one to handle dynamic allocations, where the size is unknown.
Out-of-order submissions are solved simply by having an address sent beside the
data, which are added to the beginning of that respective segment.
See \autoref{subsec:memory_segments} for a in depth explanation.
\\
To keep order of the segments, a simple directory of keys and lists
are needed. The list are ordered at insertion time, so the fist element
always is the smallest. This makes lookup to the next element constant time, and
insertions of new at most $O(n)$ time. See \autoref{subsec:dictionary}.
\\
To have the data ready, a small internal buffer is needed. This small buffer
starts filling as soon as a segment is ready.
See \autoref{subsec:memory_types}.

\subsection{Memory segments} \label{subsec:memory_segments}
The memory segment structures consists of two types with slightly different
implementations. These are called Multi memory segments and Single memory
segments.
\\Their differences are explained at the end of this section.\\
Both types consists of a lookup table, where each table entry contains
metadata, head and tail pointers. A Illustration of this can be seen in
\autoref{fig:memory_segments_explained}\\
In the implementation, the memory is not actually read, but an address is
returned. This makes it possible to use any memory type, since the latency issues
are in the submission and retrieval of the data itself, but not the calculations
for the address.\\
The lookup table consist of multiple segments references. A segment reference
contains metadata (Not illustrated in the figure) and a start and stop pointer.\\
In the illustration segments 0,1 and 4 are used, and 2 and 3 are free. Note that
segment 4 wraps around, and the segment continues.
If address 2 is requested from segment 4, memory address
15 is returned. If address 3 is requested, memory address 0 is returned.\\
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{implementation/memory_segments.eps}
	\caption{The general structure of the memory segments in use.}
	\label{fig:memory_segments_explained}
\end{figure}

\subsubsection{Multi memory segments}
Each segment has three states, indicated by two boolean values; \texttt{Done}
and \texttt{Full}. When none are \textit{true}, the segment is currently being filled
with data. When it is filled, and ready to be read, filled is marked as \textit{true},
and the data can now be read from. When the reading has ended, both \texttt{Done}
and \texttt{Full} are set to \textit{true}, and the segment is now ready to be reused in
the lookup table. See \autoref{tab:memory_states_multi}.
\begin{table}[htpb]
  \begin{center}
      \begin{tabular}{l|c|c|}
          & \tablerot{\texttt{Full}}
          & \tablerot{\texttt{Done}} \\\hline
          \texttt{Filling}  &            &            \\ \hline
          \texttt{Reading}  & \checkmark &            \\ \hline
          \texttt{Done}     & \checkmark & \checkmark \\ \hline
      \end{tabular}
  \end{center}
  \caption{The memory states of "Multi memory segments"} \label{tab:memory_states_multi}
\end{table}
To describe the operation of the memory module, all the interface functions
are listed, and their operation explained.
The memory segment interface consists of the following functions:

\begin{description}
  \item[\csharpinline{int AllocateSegment(int size)}]\hfill\\
  When a segment is allocated, the new data is saved in the segment table
  head by looking at the \texttt{Table Head} pointer.\\
  The start and end pointers are calculated based on the \texttt{Mem Head}
  pointer. If the range between \texttt{Mem Head} and \texttt{Mem Tail} is
  smaller than \csharpinline{int size}, -1 is returned, indicating
  error. If not, the segment table id is
  returned(\csharpinline{int seg_ID}).

  \item[\csharpinline{int FocusSegment()}]\hfill\\
  A segment is classified as a focus segment if the data is ready to be sent
  (\texttt{Reading} state). It will automatically find the next segment, by
  incrementing the pointer by one, and if needed wrapping back to 0. The
  \texttt{Table Tail} is also moved by this action, to always point on the first
  instance of an active segment.

  \item[\csharpinline{int SaveData(int seg_ID, int offset)}]\hfill
  Returns the memory address for that specific segment with an offset.
  This is calculated by finding the start address in the segment table, and
  adding the offset. If the segment is not in "Saving" state, an error
  is returned of -1.

  \item[\csharpinline{int LoadData(int seg_ID, int offset)}]\hfill
  The same as \csharpinline{int SaveData(...)}, with the exception
  that an error is returned if we are not in the loading state.

  \item[\csharpinline{int DelaySegment(int seg_ID)}]\hfill\\
  This function delays a focus segment by copying it to the table head, and
  freeing the current \csharpinline{int seg_ID}. When this happens, the
  chronological order of the segments gets mixed.\\
  It is now impossible to tell if the next segment in the segment table contains
  the actual continuation of the memory. The next segment may be out of order,
  and point to memory anywhere. To get around this, each block gets an
  incremental ID at creation time. Since the creation of blocks always uses
  memory from the range \texttt{Mem Head} to \texttt{Mem Tail} the memory must
  be consumed in order.

  \item[{\pbox[c][1.7\baselineskip][b]{\linewidth}{{\csharpinline{void SaveMetaData(int seg_ID, MetaData meta)}}}}]\hfill

  Saves the \csharpinline{MetaData} into the current segment.

  \item[\csharpinline{MetaData LoadMetaData(int seg_ID)}]\hfill\\
  Loads the \csharpinline{MetaData} from the current segment.

  \item[\csharpinline{void SegmentFull(int seg_ID)}]\hfill\\
  Returns if the segment is \texttt{Full}.

  \item[\csharpinline{void SegmentDone(int seg_ID)}]\hfill\\
    Returns if the segment is \texttt{Done}.
    %\notemark{Mener du "Done" her?}(See \autoref{tab:memory_states_multi}).

  \item[\csharpinline{int AllocateSegment(int size)}]\hfill\\
  Allocates a segment by getting the first available from \texttt{Table Head}.
  Returns the \csharpinline{int seg_ID} if a space is available.
  If none are available, return -1.
  %\notemark{Retur hvad?}
\end{description}




\subsubsection{Single memory segments}
The single memory segments are a bit different compared to the multi segments.
The single segments does not return an \csharpinline{int seg_ID} at any point,
because the focused segments are controlled internally. This is done to limit
the complexity of the model, and to insure that segments are not accessed
out of order. Knowing and keeping the order makes it possible to allocate
segments without knowing the size beforehand.\\
The internal segment control
consists of three pointers. A pointer for the segment that is currently being
saved to, a pointer for the segment that is currently being read from, and
a pointer to the next segment ready for allocation.\\
This model is used as the internal buffers (see \autoref{subsec:memory_types}).
This requires that the metadata can be saved before saving the data.\\
Since segments in this implementation have undefined length before being marked
as \texttt{Full}, only one segment can be filled at a time. To save metadata,
the "allocation" method from "Multi memory segments" are reused. Instead
of giving the size of the segment, the metadata is given. This marks the segment
as \texttt{Active}. An \texttt{Active} segment is ready to receive data, but only
when the currently receiving segment is done filling. See
\autoref{tab:memory_states_single} for the different states.\\

\begin{table}[htpb]
  \begin{center}
      \begin{tabular}{l|c|c|c|}
          & \tablerot{\texttt{Full}}
          & \tablerot{\texttt{Done}}
          & \tablerot{\texttt{Active}} \\\hline
          \texttt{Inactive}
          %\tablefootnote{Internal state only used to detect if too many
          %segments are allocated, filling up the segment table.}
                            &            &            &            \\ \hline
          \texttt{Filling}  &            &            & \checkmark \\ \hline
          \texttt{Reading}  & \checkmark &            & \checkmark \\ \hline
          \texttt{Done}     & \checkmark & \checkmark & \checkmark \\ \hline
      \end{tabular}
  \end{center}
  \caption{The memory states of "Single memory segments"} \label{tab:memory_states_single}
\end{table}
%\noteimprovement{Maybe add stuff like }


\subsection{Dictionary} \label{subsec:dictionary}
%\notemark{It it not a sparsly linked list, but an sparse array implemented by a linked list}
The dictionary is used to keep track of out of order memory segments.
It works by keeping two tables, one for the keys and one for the actual values.
In this model, contrary to the memory buffers, the value table actually exists
since each value needs additional data. \\
The list implementation uses a linked list, where each element consists of an
offset to the next element, and a pointer to the next element. This is used to
represent a sparse array, where only actually valid elements are saved, and
anything in between are indicated by the offset.
 In the example in
\autoref{fig:memory_dictionary_explained}, The first key "0" consists of indexes
$(1,3,5,10)$, where their respective values are saved in
addresses $(0,3,1,4)$. As an example,
if the code requests key "0" with index 5 of the linked list, address 1 is returned. If a
non existing index is requested, before being inserted, -1 is returned.
\begin{description}
  \item[\csharpinline{bool New(int key)}]\hfill\\
  When a new key is created, the key table is iterated over until a free element
  is found. If all are already used, false is returned.

  \item[\csharpinline{bool Free(int key)}]\hfill\\
  Removes a key and deletes and resets the values in their table. This will
  worst case iterate over all elements in the value table.

  \item[\csharpinline{bool ContainsKey(int key)}]\hfill\\
  Test if the key table have the key.

  \item[\csharpinline{int GetFirstValue(int key)}]\hfill\\
  Returns the first value of a key element. Finding the key in the key table takes
  n tries, but finding the first element takes constant time\footnote{Not regarding the lookup of the key table.}.

  \item[\csharpinline{int ListLength(int key)}]\hfill\\
  Get the length of the list with the offsets included. For example in
  \autoref{fig:memory_dictionary_explained}, the length of the list in
    key "0" is 10. This is calculated by adding all
    the offsets together from the key table, to the last element in the
    value table, with a pointer of -1.
    %\notemark{Markeret af Carl. Jeg kan hellere ikke helt se det fra tabellen...}

  \item[\csharpinline{int Insert(int key, int index)}]\hfill\\
  The insert creates a new entry in the value table. If it already exist, the
  same address is returned. If not, a new is returned. \\
  If a new value entry is returned, a free is found in the value table.
  The element before and after in the linked list is found , and the new element
  gets injected between those.

  \item[\csharpinline{int Delete(int key, int index)}]\hfill\\
  Delete finds the element in the list and clears it up. If none are found, -1 is
  returned. The entries in the value table before and after the deleted element
  are merged together by changing the offset and pointer in the before entry.


  \item[\csharpinline{int Observe(int key, int index)}]\hfill\\
  Observe just returns the address if it exists, or -1 if not.

  \item[{\pbox[c][1.7\baselineskip][b]{\linewidth}{{\csharpinline{void SaveMetaData(int key, MetaData meta_data)}}}}]\hfill

  The keys have space for metadata, that can be loaded and saved.

  \item[\csharpinline{MetaData LoadMetaData(int key)}]\hfill\\
  Load the metadata.

\end{description}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{implementation/memory_dictionary.eps}
	\caption{The general structure of the memory dictionary in use.}
	\label{fig:memory_dictionary_explained}
\end{figure}



\subsection{Memory types}  \label{subsec:memory_types}
Block ram on FPGA chips from companies such as Xilinx have a low latency, high
throughput, but low capacity.\cite{xilinx_fpga_memory_resources}
The capacity limitation is a problem. Worst case the
system would have to hold multiple packets of the maximum size for that specific
protocol. For example, an \gls{ipv4} packet may have a max size of 65,535
bytes.\cite{RFC0791} If a lot of packets accumulate in the system, or the user
rarely empties the \texttt{Data In} buffer, we may run out of memory.\\
An additional solution would be the to use external memory with high latency
and high capacity. The throughput would have to be equal
or bigger than the stream from the system itself to not be a bottle
neck. For example, the DDR3 ram on the
TUL PYNQ\texttrademark-Z2\cite{tul_pynq} FPGA module could be used as external memory.
This memory have a max bandwidth of 1050Mbps, which would be a bottleneck
at 10 or 100 Gbps connections. The latency of these modules are also higher
than the internal block ram.
%\notemark{DDR3 paa Pynq boarded er 1050Mbps}\\
In both cases, the memory takes at least one clock to get results back from
memory. To guarantee that the data is available as soon as possible
(see: \autoref{sec:interface_signal_protocol}) one must use prefetching of the
memory.\\
This is solvable by using small internal buffers that use the fast registers.
The size of these small buffers would be based on the latency between the
request and response from memory. This would make it easier to
replace the underlying memory, by simply increasing the buffer size to that
of the memory latency.
%\notechange{Talk about how metadata is needed before the data itself}


\section{Interface Signal protocols}
\label{sec:interface_signal_protocol}
With the introduction of buffers between each parsing processes, a clear pattern
emerged. The layer-handling, "computing", processes are responsible for numerous real-time tasks
(parsing, sending, protocol-specific tasks, etc), while also limited by their
fixed internal buffers. These processes are not always ready to receive input
from preceding processes, while they at the same time must be able to write their
output to following processes immediately.\\
The buffers are a stark opposite, as their large internal block memories enable
them to buffer huge chunks of memory, while also being able to wait for the
succeeding process to start reading.\\
With these two established scenarios, protocols for each can be proposed -- the
Buffer-Producer protocol, and the Compute-Producer protocol.

\subsection{Buffer-Producer}
The Buffer-Producer (BP) is the interface signal protocol where the producer of
the data is a buffer process (such as \texttt{Data Out} or \texttt{Segment
Out}).\\
The Buffer-Producer is heavily inspired by the Transfer signalling protocol in
the AXI4-Stream standard, which ensures a two-way flow-control mechanism for both
the producer and the consumer\footnote{The producer and consumer are called
master and slave respectively in the AXI4 specification.}\cite{arm_axi4}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{implementation/axi4_handshake.eps}
\caption{The AXI4 handshake process, adapted from “AMBA 4 AXI4 Stream-Protocol
	specification” by ARM, 2010, p. 19.\cite{arm_axi4}}
\label{fig:axi4_handshake}
\end{figure}

The AXI4-Stream protocol uses two signals (also called "flags"), the \texttt{TVALID}
on master, and \texttt{TREADY} on slave. Every time both \texttt{TVALID} and
\texttt{TREADY} are asserted during a clock-cycle, a data-exchange happens.
\autoref{fig:axi4_handshake} shows a data exchange, where the information
(the bytes) are placed on the bus and the \texttt{TVALID} is raised. When this
signal propagates to the slave, it asserts \texttt{TREADY}. When this signal
propagates back to the master, it knows that the information was read, and that
it can proceed with the next byte, or in this case, de-assert the
\texttt{TVALID} to indicate no more bytes available\cite{arm_axi4}.

The information transferred in the AXI4-Stream protocol is defined by the user,
as long as the width of the payload is an integer multiple of
bytes\cite{arm_axi4}.\\

The Buffer-Producer protocol draws heavy inspiration from this model, as it
provides a simple flow-protocol with only a few flags. However, the AXI4-Stream
protocol does not specify massive stream of data, where consecutive bytes are
sent on each clock-cycle. The issue is that, without any modifications to the
bare AXI4-Stream protocol, a producer will get notified of a value being read
by the consumer after 2 clocks. As shown on
\autoref{tab:axi4_stream_latency}, the producer has to wait 2 clocks before
updating the value on the bus, resulting in a clock in between transactions not
being utilized.


\begin{table*}[h]
  \begin{minipage}[b]{\dimexpr.5\textwidth-.5\columnsep}
    \centering
\begin{tabular}{p{1cm} | p{2.4cm} | p{2.4cm}}
\toprule
% \multicolumn{2}{c}{Item} \\
%\cmidrule(r){1-2}
		\textbf{Clock} & \textbf{Producer} & \textbf{Consumer} \\
\midrule
		0 & Puts data on bus and asserts \texttt{TVALID} &
		\texttt{TVALID} is low. NOP \\ \hline
		1 & \texttt{TREADY} is not set. Data from previous clock is
		kept on the bus & Reads data from bus and asserts \texttt{TREADY}\\ \hline
		2 & Observes that \texttt{TREADY} is asserted and updates the
		data on bus to next byte & \cellcolor{red!25} Still sees the
		old data from clock $0$!\\
\bottomrule
\end{tabular}
\caption{With the 2-clock latency for the \texttt{TVALID}/\texttt{TREADY}
	signal to propagate, the AXI4-Stream protocol cannot send consecutive
	bytes every clock.}
\label{tab:axi4_stream_latency}
\end{minipage}\hfill%
\begin{minipage}[b]{\dimexpr.5\textwidth-.5\columnsep}
\begin{tabular}{p{1cm} | p{2.4cm} | p{2.4cm}}
\toprule
% \multicolumn{2}{c}{Item} \\
%\cmidrule(r){1-2}
		\textbf{Clock} & \textbf{Producer} & \textbf{Consumer} \\
\midrule
		0 & Puts value on bus and asserts \texttt{TVALID} &
		\texttt{TVALID} is low. NOP \\ \hline
		1 & NOP & Sees that \texttt{TVALID} is high. Asserts
		\texttt{TREADY} but does not yet read the data from the bus.\\ \hline
		2 & Updates value on bus & Reads first byte \\ \hline
		3 & Updates value on bus & \cellcolor{green!25} Reads next byte \\ \hline
		n & Updates value on bus & \cellcolor{green!25} Reads n byte from the bus \\
\bottomrule
\end{tabular}
	\caption{By indicating a clock before about reading the value from the
	  bus, the 2-clock latency is avoided and the producer can update the
	  value on the bus every clock.}
\label{tab:bp_stream_latency}
\end{minipage}
\hrule
\end{table*}

This issue is circumvented in the BP protocol by asserting \texttt{TREADY} a
single clock prior, effectively indicating the intent of reading the value on
the bus during the next cycle. \autoref{tab:bp_stream_latency} shows that
even though it takes an additional clock to start a transaction, we can
circumvent the 2-clock issue that AXI4-Stream faces.\\

The BP protocol uses the same flags as those in AXI4-Stream, although slightly
differently. The \texttt{TVALID} flag is called simply \texttt{valid} in the
BP protocol. Likewise, the \texttt{TREADY} is called \texttt{ready}, however,
these both of these can be used interchangeably for clarity when comparing
against other protocols.\\

The final Buffer-Producer protocol can be summed up in these following
rules:
\begin{itemize}
	\item A data transfer only occurs a clock after both \texttt{valid} and \texttt{ready}
		are raised.
	\item When the producer has data available, it is immediately put in
		the bus and the \texttt{valid} flag is raised.
	\item Once the \texttt{valid} flag is raised, it cannot be reset until
		a data-transfer occurs.
	\item The consumer is allowed to wait until the \texttt{valid} flag is
		raised before raising the \texttt{ready} flag.
	\item If a consumer raises the \texttt{ready} flag, it is allowed to
		reset it before \texttt{valid} is raised.
\end{itemize}

The conventional data-exchange using the BP protocol in the network
stack is perhaps better visualized by a sequence diagram on
\autoref{fig:buffer_producer}.


\begin{figure}
	\centering
	\includegraphics[scale=0.5]{implementation/buffer_producer.eps}
	\caption{The usual data-transfer between a buffer (Producer) and a
	compute-process (Consumer).}
	\label{fig:buffer_producer}
\end{figure}



\subsection{Compute-Producer}
The Compute-Producer (CP) protocol is the interface signal protocol from a
compute-process to a buffer. The requirement for this protocol is that
compute-processes do not usually have the luxury of being able to wait with the
data transfer, which usually happens if the compute-process is building a
packet header or passing information along from another buffer.

The concept for the Compute-Producer model is fairly simple; since the producer
(compute-process) does not have the luxury to wait, it always sends the data
on the bus, regardless if the consumer is ready. It is up to the producer to
mark the end of an ongoing data-stream.\\

Thus, the rules for the Compute-Producer protocol are as such:
\begin{itemize}
	\item If the producer puts data on the bus, the \texttt{valid} flag
		must be raised.
	\item If \texttt{bytes\_left} is greater than $0$, the data in the next
		clock will be valid.
	\item If \texttt{bytes\_left} is $0$, the current byte ends the
		current sequence of bytes.
	\item If the consumer deasserts \texttt{ready}, it \textit{may} not read the
		data in the bus.
	\item The producer may act upon the knowledge that the consumer is
		either reading (\texttt{ready = true}) or ignoring
		(\texttt{ready = false}) the data.
\end{itemize}

Such a scenario is visualized on \autoref{fig:compute_producer}, where the
consumer becomes unavailable during the transaction. The producer has the
opportunity to drop the transaction, but it might also continue till the end.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{implementation/compute_producer.eps}
	\caption{The usual data-transfer between a compute-process (Producer) and a
	buffer (Consumer). Note that the consumer becomes unavailable halfway through the
	transaction.}
	\label{fig:compute_producer}
\end{figure}



\section{Interface Control}
The \texttt{Interface} is a collection of 3 busses provided to the user. Two of
the busses are direct connections to the data-buffer, used to transfer the
actual data to and from the network. As seen on \autoref{fig:final_design}, the
connection going to \texttt{Data In} follows the Compute-Producer interface
signal protocol, while the connection from \texttt{Data Out} follows the
Buffer-Producer protocol.\\
The connection going from the interface into the \texttt{Transport} process is
a bit more interesting. It is simply called the \texttt{InterfaceBus}, and it
is used to control the whole networking stack. As per usual, the connection
actually consists of an \texttt{InterfaceBus} controlled by the user, and the
\texttt{InterfaceControlBus}, used by the networking stack itself to respond to
the user requests.\\
Unlike the connection between buffers and processes where chunks of data are
transmitted over multiple clock-cycles, the interface connection is more of a
request-response model with only 1 clock-cycle required to submit the request
or send the response. However, after submitting the request, the user
\textit{should} keep the data in the bus until a response is received, because
the \texttt{Transport} process might be busy at the moment handling in- or
out-going packets, and not have time to process the request.\\
\autoref{fig:interfacebus_code} shows the definitions of the busses. Note
the \texttt{interface\_function} byte containing the type of "function" to
call, defined in the \texttt{InterfaceFunction enum} on
\autoref{fig:interfacefunction_code}.


\begin{figure*}[t]
    \centering

    \begin{subfigure}[b]{0.45\textwidth}
	\centering
	 %\begin{lstlisting}[language={[Sharp]C}]
\begin{mintedcsharp}
enum InterfaceFunction : byte
{
    INVALID = 0,
    // BIND = 1,
    LISTEN = 2,
    CONNECT = 3,
    ACCEPT = 4,
    CLOSE = 7,
    // ...
    OPEN = 255,
}

struct InterfaceData
{
    public int socket;
    public uint ip;
    public byte protocol;
    public ushort port;
}
\end{mintedcsharp}
%\end{lstlisting}

        \caption{Definitions of the structures used in the interface busses.}
	\label{fig:interfacefunction_code}
    \end{subfigure}%
\hfill%
    \begin{subfigure}[b]{0.45\textwidth}
	\centering
	 %\begin{lstlisting}[language={[Sharp]C}]
\begin{mintedcsharp}
interface InterfaceBus : IBus
{
    bool valid;
    byte interface_function;
    InterfaceData request;
}

interface InterfaceControlBus : IBus
{
    bool valid;

    byte exit_status;
    byte interface_function;
    InterfaceData request;
    InterfaceData response;
}
\end{mintedcsharp}
%\end{lstlisting}
	\caption{The definitions of the interface busses.\\\hfill}
	\label{fig:interfacebus_code}
    \end{subfigure}
    \caption{Pseudocode of the definitions used for the interface connection.}
	\label{fig:interface_definition}
\end{figure*}




\subsection{Usage}
The usage of this interface is very basic and primitive. The user sets the
appropriate values in the \texttt{InterfaceBus}, and raising the \texttt{valid}
flag. For example, to start listening on port $81$ using the \gls{udp}
protocol, the user would put the values on the bus as shown on
\autoref{fig:interface_request}.
\begin{figure}
\centering
\begin{Verbatim}[frame=single,samepage=true]
InterfaceBus {
  interface_function = CONNECT
  request {
    ip = 10.0.0.2
    protocol = UDP
    port = 81
  }
  valid = True
}
\end{Verbatim}
	\caption{A request on the \texttt{InterfaceBus} to connect to ip
	\texttt{10.0.0.2} using the UDP protocol on port 81.}
	\label{fig:interface_request}
\end{figure}


If the port is available under the \gls{udp} protocol and sockets are available
for allocation in the stack, the user should eventually receive a response.
On \autoref{fig:interface_response}, the user gets an \texttt{OK} response with the \texttt{socket = 2}:
\begin{figure}
	\centering
\begin{Verbatim}[frame=single,samepage=true]
InterfaceControlBus {
  valid = True
  exit_status = OK
  interface_function = CONNECT
  request {
    ip = 10.0.0.2
    protocol = UDP
    port = 81
  }
  response {
    socket = 2
  }
}
\end{Verbatim}
	\caption{A response to the request submitted in
	\autoref{fig:interface_request}. Socket "2" is returned.}
	\label{fig:interface_response}
\end{figure}
Once the user has received a valid socket, they can use the it at their leisure
for sending and receiving. \autoref{fig:interface_send_byte} shows
an example of sending the byte "A" to the newly created socket.
\begin{figure}
	\centering
\begin{Verbatim}[frame=single,samepage=true]
DataOut.WriteBus {
  socket = 2
  byte = "A"
}
DataOut.ComputeProducerBus {
  valid = True
  bytes_left = 0
}
\end{Verbatim}
	\caption{Sending the byte "A" to socket 2.}
	\label{fig:interface_send_byte}
\end{figure}
The user is not required to wait for any response, as the Compute-Producer
interface signal protocol is used.


\subsection{Limitations}
As briefly mentioned, a big limitation of the interface control is that only
one request can be proposed at a time, and the user has to wait an arbitrary
number of clocks before the response arrives. The issue is that the
\texttt{Transport} might be occupied processing in-going and out-going
packets.\\
To circumvent this, experimental features of creating a queue of requests in
the \texttt{Transport}-process was made. However, this only added complexity to
the code, increased the resource-consumption of the process by a large margin
just in order to maintain the queue data-structure, and apart from convenience,
it added no improved performance. For these reasons, the initial approach was
kept.












